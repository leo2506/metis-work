{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use newsapi to collect all pieces of news about trade war and store them in jsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T06:19:21.178741Z",
     "start_time": "2018-11-11T06:19:20.264785Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('max_colwidth',10000)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T21:22:21.488570Z",
     "start_time": "2018-11-08T21:22:21.484147Z"
    }
   },
   "outputs": [],
   "source": [
    "# A function specifies search conditions and generates an API url\n",
    "def generate_url(page,start,end):\n",
    "    url = ('https://newsapi.org/v2/everything?'\n",
    "       'q=\"trade war\"&'\n",
    "       'sources=[\"the-new-york-times\",\"cnn\",\"xinhua-net\"]&'\n",
    "       f'from={start}&'\n",
    "       f'to={end}&'\n",
    "       'sortBy=relevancy&'\n",
    "       'pageSize=100&'\n",
    "       f'page={page}&'\n",
    "       'apiKey=fc8921bcf53e482ba461714a254c0d7c'\n",
    "       )\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T21:27:18.489383Z",
     "start_time": "2018-11-08T21:27:18.483178Z"
    }
   },
   "outputs": [],
   "source": [
    "# A function takes in start and end dates of a time period,\n",
    "# and first searchs for articles within that period \n",
    "# based on the condition specified in the API url,\n",
    "# then writes the first 1000 articles of the searching result\n",
    "# into a csv file.\n",
    "def write_result(start,end):\n",
    "    \n",
    "    # Collect information about each article returned by the search\n",
    "    # Store the information in a list of jsons\n",
    "    result = []\n",
    "    for i in range(1,11):\n",
    "        response = requests.get(generate_url(i,start,end))\n",
    "        result.extend(response.json()['articles'])       \n",
    "\n",
    "    # Transport critical information of the article to a list of dictionaries\n",
    "    col = ['title','description','content','url','publishedAt']\n",
    "    df_dict = []\n",
    "    for article in result:\n",
    "        temp = {}\n",
    "        for c in col:\n",
    "            try: \n",
    "                temp[c] = article[c]\n",
    "            except:\n",
    "                temp[c] = np.nan\n",
    "        temp['source'] = article['source']['name']\n",
    "        df_dict.append(temp)\n",
    "    \n",
    "    # Convert the list of dicts into a pandas DataFrame\n",
    "    df = pd.DataFrame(df_dict,columns=col.append('source'))\n",
    "    \n",
    "    # Write the DataFrame into a csv file\n",
    "    st = ''.join(start.split('-')[1:])\n",
    "    en = ''.join(end.split('-')[1:])\n",
    "    df.to_csv(f'data/article_info({st}-{en}).csv')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T21:33:48.337592Z",
     "start_time": "2018-11-08T21:32:43.964593Z"
    }
   },
   "outputs": [],
   "source": [
    "df1 = write_result('2018-11-04','2018-11-08')\n",
    "df2 = write_result('2018-10-30','2018-11-03')\n",
    "df3 = write_result('2018-10-25','2018-10-29')\n",
    "df4 = write_result('2018-10-20','2018-10-24')\n",
    "df5 = write_result('2018-10-15','2018-10-19')\n",
    "df6 = write_result('2018-10-10','2018-10-14')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T21:47:54.289114Z",
     "start_time": "2018-11-08T21:47:54.277693Z"
    }
   },
   "outputs": [],
   "source": [
    "df_list = [df1,df2,df3,df4,df5,df6]\n",
    "df=pd.concat(df_list,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T21:48:02.104326Z",
     "start_time": "2018-11-08T21:48:02.100012Z"
    }
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T21:50:10.129934Z",
     "start_time": "2018-11-08T21:50:10.118801Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T21:52:40.427933Z",
     "start_time": "2018-11-08T21:52:40.419040Z"
    }
   },
   "outputs": [],
   "source": [
    "df.fillna('',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-09T00:41:00.652551Z",
     "start_time": "2018-11-09T00:41:00.611291Z"
    }
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T21:53:40.416569Z",
     "start_time": "2018-11-08T21:53:40.309016Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv('data/article_info_6000.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
